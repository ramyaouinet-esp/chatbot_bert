{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4maHM88EF1Vk"
      },
      "source": [
        "\n",
        "<h1 align=\"center\" style=\"color:green;font-size: 3em;\" >Building a Question Answering System Using Bert</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_VmQnqF1Vo"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<a class=\"anchor\" id=\"section2\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\">Simple Inference Pipeline on Pretrained Model</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GMnMF059F1Vo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "weight_path = \"kaporter/bert-base-uncased-finetuned-squad\"\n",
        "# loading tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(weight_path)\n",
        "#loading the model\n",
        "model = BertForQuestionAnswering.from_pretrained(weight_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quEz0T37F1Vp"
      },
      "source": [
        "Lets take an example\n",
        "\n",
        "```\n",
        "question = \"How many parameters does BERT-large have?\"\n",
        "\n",
        "context = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
        "\n",
        "answer = 340M\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdz8UkDXF1Vp",
        "outputId": "39938c86-b1ce-4f93-a511-d29c78a74c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have about 70 tokens generated\n",
            " \n",
            "Some examples of token-input_id pairs:\n",
            "[CLS] : 101\n",
            "how : 2129\n",
            "many : 2116\n",
            "parameters : 11709\n",
            "does : 2515\n",
            "bert : 14324\n",
            "- : 1011\n",
            "large : 2312\n",
            "have : 2031\n",
            "? : 1029\n",
            "[SEP] : 102\n",
            "bert : 14324\n",
            "- : 1011\n",
            "large : 2312\n",
            "is : 2003\n",
            "really : 2428\n",
            "big : 2502\n",
            ". : 1012\n",
            ". : 1012\n",
            ". : 1012\n",
            "it : 2009\n",
            "has : 2038\n",
            "24 : 2484\n",
            "- : 1011\n",
            "layers : 9014\n",
            "and : 1998\n",
            "an : 2019\n",
            "em : 7861\n",
            "##bed : 8270\n",
            "##ding : 4667\n",
            "size : 2946\n",
            "of : 1997\n",
            "1 : 1015\n",
            ", : 1010\n",
            "02 : 6185\n",
            "##4 : 2549\n",
            ", : 1010\n",
            "for : 2005\n",
            "a : 1037\n",
            "total : 2561\n",
            "of : 1997\n",
            "340 : 16029\n",
            "##m : 2213\n",
            "parameters : 11709\n",
            "! : 999\n",
            "altogether : 10462\n",
            "it : 2009\n",
            "is : 2003\n",
            "1 : 1015\n",
            ". : 1012\n",
            "34 : 4090\n",
            "##gb : 18259\n",
            ", : 1010\n",
            "so : 2061\n",
            "expect : 5987\n",
            "it : 2009\n",
            "to : 2000\n",
            "take : 2202\n",
            "a : 1037\n",
            "couple : 3232\n",
            "minutes : 2781\n",
            "to : 2000\n",
            "download : 8816\n",
            "to : 2000\n",
            "your : 2115\n",
            "cola : 15270\n",
            "##b : 2497\n",
            "instance : 6013\n",
            ". : 1012\n",
            "[SEP] : 102\n"
          ]
        }
      ],
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "context = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
        "\n",
        "input_ids = tokenizer.encode(question, context)\n",
        "print (f'We have about {len(input_ids)} tokens generated')\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(\" \")\n",
        "print('Some examples of token-input_id pairs:')\n",
        "\n",
        "for i, (token,inp_id) in enumerate(zip(tokens,input_ids)):\n",
        "    print(token,\":\",inp_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXLv0JfWF1Vq",
        "outputId": "f50ec442-dcd8-4555-f27e-c178f30f1d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "sep_idx = tokens.index('[SEP]')\n",
        "\n",
        "# we will provide including [SEP] token which seperates question from context and 1 for rest.\n",
        "token_type_ids = [0 for i in range(sep_idx+1)] + [1 for i in range(sep_idx+1,len(tokens))]\n",
        "print(token_type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8zc4J9pF1Vq"
      },
      "source": [
        "Now lets pass our input through model and sees the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McRl6K99F1Vq",
        "outputId": "72532deb-7f7a-4487-ab91-2331ff84e7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted answer: 340\n"
          ]
        }
      ],
      "source": [
        "# Run our example through the model.\n",
        "out = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                token_type_ids=torch.tensor([token_type_ids]))\n",
        "\n",
        "start_logits,end_logits = out['start_logits'],out['end_logits']\n",
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_logits)\n",
        "answer_end = torch.argmax(end_logits)\n",
        "\n",
        "ans = ''.join(tokens[answer_start:answer_end])\n",
        "print('Predicted answer:', ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TDkn-4gPF1Vq"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "del tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKfibNLtF1Vq"
      },
      "source": [
        "We have seen that how we can predict using a finetuned bert(bert-base) model. Now lets train and model on Squad dataest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1KQMoHXPF1Vq"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1L3uUL2F1Vr"
      },
      "source": [
        "**Loading dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOlud1YXRBL_",
        "outputId": "160d00de-729c-480a-93a2-b20733763d0f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGNWyOFzF1Vr",
        "outputId": "6e411cb6-a936-4f1e-da3f-021286f9631f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"rajpurkar/squad\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8unlfvyQF1Vr"
      },
      "source": [
        "We have about 87599 data points in train and 10570 in validation.\n",
        "Lets see some sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OcB3AmMSF1Vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0bef64-b0c0-4952-c411-bc28a8606cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mTrain Data Sample.....\u001b[0;0m\n",
            " \n",
            "\u001b[1mID -\u001b[0;0m 5733be284776f41900661182\n",
            "\u001b[1mTITLE - \u001b[0;0m University_of_Notre_Dame\n",
            "\u001b[1mCONTEXT - \u001b[0;0m Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "\u001b[1mANSWERS - \u001b[0;0m ['Saint Bernadette Soubirous']\n",
            "\u001b[1mANSWERS START INDEX - \u001b[0;0m [515]\n",
            " \n",
            "------------------------------------------------------------------------------------------\n",
            "\u001b[1mValidation Data Sample.....\u001b[0;0m\n",
            " \n",
            "\u001b[1mID -\u001b[0;0m 56be4db0acb8001400a502ec\n",
            "\u001b[1mTITLE - \u001b[0;0m Super_Bowl_50\n",
            "\u001b[1mCONTEXT - \u001b[0;0m Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "\u001b[1mANSWERS - \u001b[0;0m ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
            "\u001b[1mANSWERS START INDEX - \u001b[0;0m [177, 177, 177]\n",
            " \n"
          ]
        }
      ],
      "source": [
        "# to make text bold\n",
        "s_bold = '\\033[1m'\n",
        "e_bold = '\\033[0;0m'\n",
        "\n",
        "print(s_bold + 'Train Data Sample.....' + e_bold)\n",
        "train_data = dataset[\"train\"]\n",
        "for data in train_data:\n",
        "    print(' ')\n",
        "    print(s_bold + 'ID -' + e_bold, data['id'])\n",
        "    print(s_bold +'TITLE - '+ e_bold, data['title'])\n",
        "    print(s_bold + 'CONTEXT - '+ e_bold,data['context'])\n",
        "    print(s_bold + 'ANSWERS - ' + e_bold,data['answers']['text'])\n",
        "    print(s_bold + 'ANSWERS START INDEX - ' + e_bold,data['answers']['answer_start'])\n",
        "    print(' ')\n",
        "    break\n",
        "\n",
        "print('---'*30)\n",
        "print(s_bold + 'Validation Data Sample.....' + e_bold)\n",
        "train_data = dataset[\"validation\"]\n",
        "for data in train_data:\n",
        "    print(' ')\n",
        "    print(s_bold + 'ID -' + e_bold, data['id'])\n",
        "    print(s_bold +'TITLE - '+ e_bold, data['title'])\n",
        "    print(s_bold + 'CONTEXT - '+ e_bold,data['context'])\n",
        "    print(s_bold + 'ANSWERS - ' + e_bold,data['answers']['text'])\n",
        "    print(s_bold + 'ANSWERS START INDEX - ' + e_bold,data['answers']['answer_start'])\n",
        "    print(' ')\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJVt1vDYF1Vr"
      },
      "source": [
        "Another important thing that we found is that we can see multiple answers in one of our validation sample.Lets analyze it further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FHhYl8fnF1Vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f066f2-0642-486a-f4e4-74a6b38e7750"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 0\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "dataset[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oEG3Oh3sF1Vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167b6ad5-2e4d-4553-eaa7-fd6a18e43c00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 10567\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "dataset[\"validation\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOt75zSmF1Vs"
      },
      "source": [
        "We can see that in train we have only one answer for all the samples.But in validation data there are 10567 samples with multiple answers.\n",
        "\n",
        "Before getting to the problem Let us understand how a question answering problem is solved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Up5PKXQXF1Vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fecf3aaf-b7f1-4939-8628-860bd6159498"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "## Lets sample some dataset so that we can reduce training time.\n",
        "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(8000)])\n",
        "dataset[\"validation\"] = dataset[\"validation\"].select([i for i in range(2000)])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh8rEZrKF1Vs"
      },
      "source": [
        "**Data Preprocessing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-GimsXGvF1Vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de832203-2a43-4839-bd03-827a9c0eb970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 4 examples gave 2 features.\n",
            "Here is where each comes from: [0, 0].\n",
            "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            " \n",
            "Context :  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer:  ['Saint Bernadette Soubirous']\n",
            "--------------------------------------------------\n",
            "Context piece 1\n",
            "[SEP] architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues [SEP]\n",
            " \n",
            "Context piece 2\n",
            "[SEP] sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP]\n",
            " \n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# model_checkpoint = \"bert-base-cased\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "context = dataset[\"train\"][0][\"context\"]\n",
        "question = dataset[\"train\"][0][\"question\"]\n",
        "answer = dataset[\"train\"][0][\"answers\"][\"text\"]\n",
        "\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length=160,\n",
        "    truncation=\"only_second\",  # only to truncate context\n",
        "    stride=70,  # no of overlapping tokens  between concecute context pieces\n",
        "    return_overflowing_tokens=True,  #to let tokenizer know we want overflow tokens\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
        "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")\n",
        "\n",
        "print('Question: ',question)\n",
        "print(' ')\n",
        "print('Context : ',context)\n",
        "print(' ')\n",
        "print('Answer: ', answer)\n",
        "print('--'*25)\n",
        "\n",
        "for i,ids in enumerate(inputs[\"input_ids\"]):\n",
        "    print('Context piece', i+1)\n",
        "    print(tokenizer.decode(ids[ids.index(102):]))\n",
        "    print(' ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NZhoGa5fF1Vt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a692cdc1-b19f-46b8-9165-94dc43fda6a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "del tokenizer\n",
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "def train_data_preprocess(examples):\n",
        "\n",
        "    \"\"\"\n",
        "    generate start and end indexes of answer in context\n",
        "    \"\"\"\n",
        "\n",
        "    def find_context_start_end_index(sequence_ids):\n",
        "        \"\"\"\n",
        "        returns the token index in whih context starts and ends\n",
        "        \"\"\"\n",
        "        token_idx = 0\n",
        "        while sequence_ids[token_idx] != 1:  #means its special tokens or tokens of queston\n",
        "            token_idx += 1                   # loop only break when context starts in tokens\n",
        "        context_start_idx = token_idx\n",
        "\n",
        "        while sequence_ids[token_idx] == 1:\n",
        "            token_idx += 1\n",
        "        context_end_idx = token_idx - 1\n",
        "        return context_start_idx,context_end_idx\n",
        "\n",
        "\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    context = examples[\"context\"]\n",
        "    answers = examples[\"answers\"]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        context,\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,  #returns id of base context\n",
        "        return_offsets_mapping=True,  # returns (start_index,end_index) of each token\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "\n",
        "    for i,mapping_idx_pairs in enumerate(inputs['offset_mapping']):\n",
        "        context_idx = inputs['overflow_to_sample_mapping'][i]\n",
        "\n",
        "        # from main context\n",
        "        answer = answers[context_idx]\n",
        "        answer_start_char_idx = answer['answer_start'][0]\n",
        "        answer_end_char_idx = answer_start_char_idx + len(answer['text'][0])\n",
        "\n",
        "\n",
        "        # now we have to find it in sub contexts\n",
        "        tokens = inputs['input_ids'][i]\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # finding the context start and end indexes wrt sub context tokens\n",
        "        context_start_idx,context_end_idx = find_context_start_end_index(sequence_ids)\n",
        "\n",
        "        #if the answer is not fully inside context label it as (0,0)\n",
        "        # starting and end index of charecter of full context text\n",
        "        context_start_char_index = mapping_idx_pairs[context_start_idx][0]\n",
        "        context_end_char_index = mapping_idx_pairs[context_end_idx][1]\n",
        "\n",
        "\n",
        "        #If the answer is not fully inside the context, label is (0, 0)\n",
        "        if (context_start_char_index > answer_start_char_idx) or (\n",
        "            context_end_char_index < answer_end_char_idx):\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # else its start and end token positions\n",
        "            # here idx indicates index of token\n",
        "            idx = context_start_idx\n",
        "            while idx <= context_end_idx and mapping_idx_pairs[idx][0] <= answer_start_char_idx:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "\n",
        "            idx = context_end_idx\n",
        "            while idx >= context_start_idx and mapping_idx_pairs[idx][1] > answer_end_char_idx:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "train_sample = dataset[\"train\"].select([i for i in range(200)])\n",
        "\n",
        "train_dataset = train_sample.map(\n",
        "    train_data_preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "len(dataset[\"train\"]),len(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgCb4VUHF1Vt"
      },
      "source": [
        "We can see a increase in number of datapoints after the tokenization method we used.Lets compare the values before and after tokenization.We will print some of the questions,context and answers after tokenization and compare with the original one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "O-RzabrPF1Vt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427019c3-06bb-4633-f3a9-35a9d408b9be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['Saint Bernadette Soubirous']\n",
            " \n",
            "Start and end index of text:  515 541\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "saint bernadette soubirous\n",
            " \n",
            "Start pos and end pos of tokens:  130 138\n",
            "________________________________________________________________________________\n",
            "1\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "What is in front of the Notre Dame Main Building?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['a copper statue of Christ']\n",
            " \n",
            "Start and end index of text:  188 213\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] what is in front of the notre dame main building? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "a copper statue of christ\n",
            " \n",
            "Start pos and end pos of tokens:  52 57\n",
            "________________________________________________________________________________\n",
            "2\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['the Main Building']\n",
            " \n",
            "Start and end index of text:  279 296\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] the basilica of the sacred heart at notre dame is beside to which structure? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "the main building\n",
            " \n",
            "Start pos and end pos of tokens:  81 84\n",
            "________________________________________________________________________________\n",
            "3\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "What is the Grotto at Notre Dame?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['a Marian place of prayer and reflection']\n",
            " \n",
            "Start and end index of text:  381 420\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] what is the grotto at notre dame? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "a marian place of prayer and reflection\n",
            " \n",
            "Start pos and end pos of tokens:  95 102\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def print_context_and_answer(idx,mini_ds=dataset[\"train\"]):\n",
        "\n",
        "    print(idx)\n",
        "    print('----')\n",
        "    question = mini_ds[idx]['question']\n",
        "    context = mini_ds[idx]['context']\n",
        "    answer = mini_ds[idx]['answers']['text']\n",
        "    print('Theoretical values :')\n",
        "    print(' ')\n",
        "    print('Question: ')\n",
        "    print(question)\n",
        "    print(' ')\n",
        "    print('Context: ')\n",
        "    print(context)\n",
        "    print(' ')\n",
        "    print('Answer: ')\n",
        "    print(answer)\n",
        "    print(' ')\n",
        "    answer_start_char_idx = mini_ds[idx]['answers']['answer_start'][0]\n",
        "    answer_end_char_idx = answer_start_char_idx + len(mini_ds[idx]['answers']['text'][0])\n",
        "    print('Start and end index of text: ',answer_start_char_idx,answer_end_char_idx)\n",
        "    print('----'*20)\n",
        "    print('Values after tokenization:')\n",
        "\n",
        "\n",
        "    #answer\n",
        "    sep_tok_index = train_dataset[idx]['input_ids'].index(102) #get index for [SEP]\n",
        "    question_ = train_dataset[idx]['input_ids'][:sep_tok_index+1]\n",
        "    question_decoded = tokenizer.decode(question_)\n",
        "    context_ = train_dataset[idx]['input_ids'][sep_tok_index+1:]\n",
        "    context_decoded = tokenizer.decode(context_)\n",
        "    start_idx = train_dataset[idx]['start_positions']\n",
        "    end_idx = train_dataset[idx]['end_positions']\n",
        "    answer_toks = train_dataset[idx]['input_ids'][start_idx:end_idx]\n",
        "    answer_decoded = tokenizer.decode(answer_toks)\n",
        "    print(' ')\n",
        "    print('Question: ')\n",
        "    print(question_decoded)\n",
        "    print(' ')\n",
        "    print('Context: ')\n",
        "    print(context_decoded)\n",
        "    print(' ')\n",
        "    print('Answer: ')\n",
        "    print(answer_decoded)\n",
        "    print(' ')\n",
        "    print('Start pos and end pos of tokens: ',train_dataset[idx]['start_positions'],train_dataset[idx]['end_positions'])\n",
        "    print('____'*20)\n",
        "\n",
        "\n",
        "print_context_and_answer(0)\n",
        "print_context_and_answer(1)\n",
        "print_context_and_answer(2)\n",
        "print_context_and_answer(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQsUWj2PF1Vu"
      },
      "source": [
        "\n",
        "\n",
        "<a class=\"anchor\" id=\"section4\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\">Understanding Metrics needed for Evaluation</h2>\n",
        "\n",
        "**How to evaluate the model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEv1HXMZF1Vu"
      },
      "source": [
        "Lets take a small eval set.Here we donot need to do much preprocesing. . We will use the pretrained \"distilbert-base-uncased\" weights which is not fine tuned and lets see how the model performs.\n",
        "\n",
        "* We will set offset to None for all those questions part of the data.\n",
        "* We will also append base context id to each sample\n",
        "\n",
        "\n",
        "We will evalue using our untuned bert-base model and lets see the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0LQ25OHgF1Vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1f3a22-1d30-46e5-b692-5c0871dbcd53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def preprocess_validation_examples(examples):\n",
        "    \"\"\"\n",
        "    preprocessing validation data\n",
        "    \"\"\"\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    base_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "\n",
        "        # take the base id (ie in cases of overflow happens we get base id)\n",
        "        base_context_idx = sample_map[i]\n",
        "        base_ids.append(examples[\"id\"][base_context_idx])\n",
        "\n",
        "        # sequence id indicates the input. 0 for first input and 1 for second input\n",
        "        # and None for special tokens by default\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        # for Question tokens provide offset_mapping as None\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"base_id\"] = base_ids\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# del tokenizer\n",
        "\n",
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "data_val_sample = dataset[\"validation\"].select([i for i in range(100)])\n",
        "eval_set = data_val_sample.map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"validation\"].column_names,\n",
        ")\n",
        "len(eval_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "WwA1cAfEF1Vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eedbdcb-2589-4f5d-ed17-104f099e902d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 512), (100, 512))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "\n",
        "# del tokenizer\n",
        "# take a small sample\n",
        "\n",
        "eval_set_for_model = eval_set.remove_columns([\"base_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")\n",
        "\n",
        "checkpoint =  \"distilbert-base-uncased\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(checkpoint).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**batch)\n",
        "\n",
        "start_logits = outputs.start_logits.cpu().numpy()\n",
        "end_logits = outputs.end_logits.cpu().numpy()\n",
        "\n",
        "start_logits.shape,end_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6rLZE8lF1Vu"
      },
      "source": [
        "We will evaluate our model using Evaluate library. We use 2 metrics for evaluation.\n",
        "\n",
        "1. Exact match\n",
        "2. f1 score\n",
        "\n",
        "**Exact Match**\n",
        "\n",
        "For each question-answer pair if the charecters of the models prediction exactly match with charecters of true answer then EM=1 else 0.When assessing against a negative example, if the model predicts any text at all, it automatically receives a 0 for that example\n",
        "\n",
        "**F1 score**\n",
        "\n",
        "F1 score depends up on precision and recall.\n",
        "```\n",
        "f1 score = 2 * (precision * recall)/ precision + recall\n",
        "\n",
        "```\n",
        "\n",
        "If we take the theoritical answers and predicted answers,the number of shared words between theoritical and predicted answer is the basis for f1 score.precision is the ratio of the number of shared words to the total number of words in the prediction, and recall is the ratio of the number of shared words to the total number of words in the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ydxDeFzYF1Vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81ef12e-d20e-4ac7-b739-b87e9d1792a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5xYoMvtqF1Vv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import evaluate\n",
        "\n",
        "def predict_answers_and_evaluate(start_logits,end_logits,eval_set,examples):\n",
        "    \"\"\"\n",
        "    make predictions\n",
        "    Args:\n",
        "    start_logits : strat_position prediction logits\n",
        "    end_logits: end_position prediction logits\n",
        "    eval_set: processed val data\n",
        "    examples: unprocessed val data with context text\n",
        "    \"\"\"\n",
        "    # appending all id's corresponding to the base context id\n",
        "    example_to_features = collections.defaultdict(list)\n",
        "    for idx, feature in enumerate(eval_set):\n",
        "        example_to_features[feature[\"base_id\"]].append(idx)\n",
        "\n",
        "    n_best = 20\n",
        "    max_answer_length = 30\n",
        "    predicted_answers = []\n",
        "\n",
        "    for example in examples:\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "\n",
        "        # looping through each sub contexts corresponding to a context and finding\n",
        "        # answers\n",
        "        for feature_index in example_to_features[example_id]:\n",
        "            start_logit = start_logits[feature_index]\n",
        "            end_logit = end_logits[feature_index]\n",
        "            offsets = eval_set[\"offset_mapping\"][feature_index]\n",
        "\n",
        "            # sorting the predictions of all hidden states and taking best n_best prediction\n",
        "            # means taking the index of top 20 tokens\n",
        "            start_indexes = np.argsort(start_logit).tolist()[::-1][:n_best]\n",
        "            end_indexes = np.argsort(end_logit).tolist()[::-1][:n_best]\n",
        "\n",
        "\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "\n",
        "                    # Skip answers that are not fully in the context\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                       ):\n",
        "                        continue\n",
        "\n",
        "                    answers.append({\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                        })\n",
        "\n",
        "\n",
        "            # Select the answer with the best score\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "            predicted_answers.append(\n",
        "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
        "            )\n",
        "        else:\n",
        "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
        "\n",
        "    metric = evaluate.load(\"squad\")\n",
        "\n",
        "    theoretical_answers = [\n",
        "            {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples\n",
        "    ]\n",
        "\n",
        "    metric_ = metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
        "    return predicted_answers,metric_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PisVBthF1Vv"
      },
      "source": [
        "Let us evaluate the model.This metric expects the predicted answers in the format we saw above (a list of dictionaries with one key for the ID of the example and one key for the predicted text) and the theoretical answers in the format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-Qbj481fF1Vv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7514555b-99b4-449d-f221-2515e05a7fe3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 2.0, 'f1': 5.878787878787878}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "pred_answers,metrics_ = predict_answers_and_evaluate(start_logits,end_logits,eval_set,data_val_sample)\n",
        "metrics_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z395hXt9F1Vv"
      },
      "source": [
        "We have very poor score as expected. Now we will fine tune the model and will see the performance in whole validation dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oDEDkkGF1Vv"
      },
      "source": [
        "\n",
        "<a class=\"anchor\" id=\"section5\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\">Training a Question Answering System based on Bert</h2>\n",
        "\n",
        "Lets again load the dataset from fresh. We will sample a small portion of dataset for training. You can train with full data if you have enough resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RCtfIvOUF1Vv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee70c9f9-74a8-4e91-8021-43586bb8d795"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "#lets sample a small dataset\n",
        "dataset['train'] = dataset['train'].select([i for i in range(5000)])\n",
        "dataset['validation'] = dataset['validation'].select([i for i in range(500)])\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYNJc-xEF1Vv"
      },
      "source": [
        "Let us define a Pyorch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "F_7MzgvBF1Vv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class DataQA(Dataset):\n",
        "    def __init__(self, dataset,mode=\"train\"):\n",
        "        self.mode = mode\n",
        "\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            # sampling\n",
        "            self.dataset = dataset[\"train\"]\n",
        "            self.data = self.dataset.map(train_data_preprocess,\n",
        "                                                      batched=True,\n",
        "                            remove_columns= dataset[\"train\"].column_names)\n",
        "\n",
        "        else:\n",
        "            self.dataset = dataset[\"validation\"]\n",
        "            self.data = self.dataset.map(preprocess_validation_examples,\n",
        "            batched=True,remove_columns = dataset[\"validation\"].column_names,\n",
        "               )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        out = {}\n",
        "        example = self.data[idx]\n",
        "        out['input_ids'] = torch.tensor(example['input_ids'])\n",
        "        out['attention_mask'] = torch.tensor(example['attention_mask'])\n",
        "\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "\n",
        "            out['start_positions'] = torch.unsqueeze(torch.tensor(example['start_positions']),dim=0)\n",
        "            out['end_positions'] = torch.unsqueeze(torch.tensor(example['end_positions']),dim=0)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "bBjFyd9dF1Vw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635,
          "referenced_widgets": [
            "017f094634cd447d936595b0ade9d42e",
            "40609dfb82dc4de9a5961c9a4fcbef1b",
            "e6fea980df334b52a6aaf670e1fbf5a8",
            "c43f416ba6dd4a0caa9c12f0e71b30df",
            "cfba5bf272a945568842483b95782ea7",
            "1f7a2a57b9f74d7aa0b25c8af5e67608",
            "1b8c5292fa2c4a83854d91f26b104dac",
            "0d65c68975964edbbb8a1cc40c164846",
            "f8f02dd53b8a4714a7cb2cce1eb831d0",
            "eb0608734b154bf58858c5c2707ad00f",
            "fc112def60624c37be1d287ba61f0ff8"
          ]
        },
        "outputId": "6b056219-f075-456d-956b-d0c0795177e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "017f094634cd447d936595b0ade9d42e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "____________________________________________________________________________________________________\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "\n",
        "train_dataset = DataQA(dataset,mode=\"train\")\n",
        "val_dataset = DataQA(dataset,mode=\"validation\")\n",
        "\n",
        "\n",
        "\n",
        "for i,d in enumerate(train_dataset):\n",
        "    for k in d.keys():\n",
        "        print(k + ' : ', d[k].shape)\n",
        "    print('--'*40)\n",
        "\n",
        "    if i == 3:\n",
        "        break\n",
        "\n",
        "print('__'*50)\n",
        "\n",
        "for i,d in enumerate(val_dataset):\n",
        "    for k in d.keys():\n",
        "        print(k + ' : ', len(d[k]))\n",
        "    print('--'*40)\n",
        "\n",
        "    if i == 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTD_lkgWF1Vw"
      },
      "source": [
        "Let us load the data in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ti7f6o4UF1Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f146115c-e6e9-4a60-972e-40ed0f753a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512])\n",
            "torch.Size([2, 512])\n",
            "torch.Size([2, 1])\n",
            "torch.Size([2, 1])\n",
            "------------------------------------------------------------\n",
            "torch.Size([2, 512])\n",
            "torch.Size([2, 512])\n"
          ]
        }
      ],
      "source": [
        "from transformers import default_data_collator\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=default_data_collator,\n",
        "    batch_size=2,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    val_dataset, collate_fn=default_data_collator, batch_size=2\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for batch in train_dataloader:\n",
        "   print(batch['input_ids'].shape)\n",
        "   print(batch['attention_mask'].shape)\n",
        "   print(batch['start_positions'].shape)\n",
        "   print(batch['end_positions'].shape)\n",
        "   break\n",
        "\n",
        "print('---'*20)\n",
        "\n",
        "for batch in eval_dataloader:\n",
        "   print(batch['input_ids'].shape)\n",
        "   print(batch['attention_mask'].shape)\n",
        "   break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KefZApbiF1Vw"
      },
      "source": [
        "**Define Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "U4a4-E7gF1Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14363f3-0cac-48c2-aaff-ea320885ca8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertForQuestionAnswering\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Available device: {device}')\n",
        "\n",
        "checkpoint =  \"distilbert-base-uncased\"\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(checkpoint)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS86NEmpF1Vw"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Gv3N1xB0F1Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f1ecc4-634f-46db-d0d7-4f1cb699c758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5010\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import evaluate\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "print(total_steps)\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siYs04aCF1Vw"
      },
      "source": [
        "We need processed validation data at the time of evaluation to get offsets for each context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "SroZjZaNF1Vw"
      },
      "outputs": [],
      "source": [
        "# we need processed validation data to get offsets at the time of evaluation\n",
        "validation_processed_dataset = dataset[\"validation\"].map(preprocess_validation_examples,\n",
        "            batched=True,remove_columns = dataset[\"validation\"].column_names,\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4PE9GB3DF1Vx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04e981c-ba77-4959-f00b-cd2ff3315c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "=====Epoch 1=====\n",
            "Training....\n",
            "  Batch    40  of  2,505.    Elapsed: 0:00:04.\n",
            "  Batch    80  of  2,505.    Elapsed: 0:00:09.\n",
            "  Batch   120  of  2,505.    Elapsed: 0:00:13.\n",
            "  Batch   160  of  2,505.    Elapsed: 0:00:18.\n",
            "  Batch   200  of  2,505.    Elapsed: 0:00:22.\n",
            "  Batch   240  of  2,505.    Elapsed: 0:00:27.\n",
            "  Batch   280  of  2,505.    Elapsed: 0:00:31.\n",
            "  Batch   320  of  2,505.    Elapsed: 0:00:36.\n",
            "  Batch   360  of  2,505.    Elapsed: 0:00:40.\n",
            "  Batch   400  of  2,505.    Elapsed: 0:00:44.\n",
            "  Batch   440  of  2,505.    Elapsed: 0:00:49.\n",
            "  Batch   480  of  2,505.    Elapsed: 0:00:53.\n",
            "  Batch   520  of  2,505.    Elapsed: 0:00:58.\n",
            "  Batch   560  of  2,505.    Elapsed: 0:01:02.\n",
            "  Batch   600  of  2,505.    Elapsed: 0:01:06.\n",
            "  Batch   640  of  2,505.    Elapsed: 0:01:11.\n",
            "  Batch   680  of  2,505.    Elapsed: 0:01:15.\n",
            "  Batch   720  of  2,505.    Elapsed: 0:01:20.\n",
            "  Batch   760  of  2,505.    Elapsed: 0:01:24.\n",
            "  Batch   800  of  2,505.    Elapsed: 0:01:29.\n",
            "  Batch   840  of  2,505.    Elapsed: 0:01:33.\n",
            "  Batch   880  of  2,505.    Elapsed: 0:01:37.\n",
            "  Batch   920  of  2,505.    Elapsed: 0:01:42.\n",
            "  Batch   960  of  2,505.    Elapsed: 0:01:47.\n",
            "  Batch 1,000  of  2,505.    Elapsed: 0:01:51.\n",
            "  Batch 1,040  of  2,505.    Elapsed: 0:01:55.\n",
            "  Batch 1,080  of  2,505.    Elapsed: 0:02:00.\n",
            "  Batch 1,120  of  2,505.    Elapsed: 0:02:04.\n",
            "  Batch 1,160  of  2,505.    Elapsed: 0:02:09.\n",
            "  Batch 1,200  of  2,505.    Elapsed: 0:02:13.\n",
            "  Batch 1,240  of  2,505.    Elapsed: 0:02:18.\n",
            "  Batch 1,280  of  2,505.    Elapsed: 0:02:23.\n",
            "  Batch 1,320  of  2,505.    Elapsed: 0:02:27.\n",
            "  Batch 1,360  of  2,505.    Elapsed: 0:02:32.\n",
            "  Batch 1,400  of  2,505.    Elapsed: 0:02:36.\n",
            "  Batch 1,440  of  2,505.    Elapsed: 0:02:41.\n",
            "  Batch 1,480  of  2,505.    Elapsed: 0:02:45.\n",
            "  Batch 1,520  of  2,505.    Elapsed: 0:02:50.\n",
            "  Batch 1,560  of  2,505.    Elapsed: 0:02:54.\n",
            "  Batch 1,600  of  2,505.    Elapsed: 0:02:59.\n",
            "  Batch 1,640  of  2,505.    Elapsed: 0:03:04.\n",
            "  Batch 1,680  of  2,505.    Elapsed: 0:03:08.\n",
            "  Batch 1,720  of  2,505.    Elapsed: 0:03:13.\n",
            "  Batch 1,760  of  2,505.    Elapsed: 0:03:17.\n",
            "  Batch 1,800  of  2,505.    Elapsed: 0:03:22.\n",
            "  Batch 1,840  of  2,505.    Elapsed: 0:03:27.\n",
            "  Batch 1,880  of  2,505.    Elapsed: 0:03:31.\n",
            "  Batch 1,920  of  2,505.    Elapsed: 0:03:36.\n",
            "  Batch 1,960  of  2,505.    Elapsed: 0:03:41.\n",
            "  Batch 2,000  of  2,505.    Elapsed: 0:03:45.\n",
            "  Batch 2,040  of  2,505.    Elapsed: 0:03:50.\n",
            "  Batch 2,080  of  2,505.    Elapsed: 0:03:55.\n",
            "  Batch 2,120  of  2,505.    Elapsed: 0:03:59.\n",
            "  Batch 2,160  of  2,505.    Elapsed: 0:04:04.\n",
            "  Batch 2,200  of  2,505.    Elapsed: 0:04:08.\n",
            "  Batch 2,240  of  2,505.    Elapsed: 0:04:13.\n",
            "  Batch 2,280  of  2,505.    Elapsed: 0:04:18.\n",
            "  Batch 2,320  of  2,505.    Elapsed: 0:04:22.\n",
            "  Batch 2,360  of  2,505.    Elapsed: 0:04:27.\n",
            "  Batch 2,400  of  2,505.    Elapsed: 0:04:32.\n",
            "  Batch 2,440  of  2,505.    Elapsed: 0:04:36.\n",
            "  Batch 2,480  of  2,505.    Elapsed: 0:04:41.\n",
            "\n",
            "  Average training loss: 2.33\n",
            "  Training epoch took: 0:04:44\n",
            "\n",
            "Running Validation...\n",
            "Exact match: 23.4, F1 score: 59.22810318680287\n",
            "\n",
            "  Validation took: 0:04:14\n",
            " \n",
            "=====Epoch 2=====\n",
            "Training....\n",
            "  Batch    40  of  2,505.    Elapsed: 0:00:04.\n",
            "  Batch    80  of  2,505.    Elapsed: 0:00:09.\n",
            "  Batch   120  of  2,505.    Elapsed: 0:00:13.\n",
            "  Batch   160  of  2,505.    Elapsed: 0:00:17.\n",
            "  Batch   200  of  2,505.    Elapsed: 0:00:21.\n",
            "  Batch   240  of  2,505.    Elapsed: 0:00:26.\n",
            "  Batch   280  of  2,505.    Elapsed: 0:00:30.\n",
            "  Batch   320  of  2,505.    Elapsed: 0:00:34.\n",
            "  Batch   360  of  2,505.    Elapsed: 0:00:39.\n",
            "  Batch   400  of  2,505.    Elapsed: 0:00:43.\n",
            "  Batch   440  of  2,505.    Elapsed: 0:00:48.\n",
            "  Batch   480  of  2,505.    Elapsed: 0:00:52.\n",
            "  Batch   520  of  2,505.    Elapsed: 0:00:56.\n",
            "  Batch   560  of  2,505.    Elapsed: 0:01:01.\n",
            "  Batch   600  of  2,505.    Elapsed: 0:01:05.\n",
            "  Batch   640  of  2,505.    Elapsed: 0:01:10.\n",
            "  Batch   680  of  2,505.    Elapsed: 0:01:14.\n",
            "  Batch   720  of  2,505.    Elapsed: 0:01:19.\n",
            "  Batch   760  of  2,505.    Elapsed: 0:01:23.\n",
            "  Batch   800  of  2,505.    Elapsed: 0:01:27.\n",
            "  Batch   840  of  2,505.    Elapsed: 0:01:32.\n",
            "  Batch   880  of  2,505.    Elapsed: 0:01:36.\n",
            "  Batch   920  of  2,505.    Elapsed: 0:01:41.\n",
            "  Batch   960  of  2,505.    Elapsed: 0:01:45.\n",
            "  Batch 1,000  of  2,505.    Elapsed: 0:01:50.\n",
            "  Batch 1,040  of  2,505.    Elapsed: 0:01:54.\n",
            "  Batch 1,080  of  2,505.    Elapsed: 0:01:59.\n",
            "  Batch 1,120  of  2,505.    Elapsed: 0:02:03.\n",
            "  Batch 1,160  of  2,505.    Elapsed: 0:02:08.\n",
            "  Batch 1,200  of  2,505.    Elapsed: 0:02:12.\n",
            "  Batch 1,240  of  2,505.    Elapsed: 0:02:17.\n",
            "  Batch 1,280  of  2,505.    Elapsed: 0:02:21.\n",
            "  Batch 1,320  of  2,505.    Elapsed: 0:02:26.\n",
            "  Batch 1,360  of  2,505.    Elapsed: 0:02:31.\n",
            "  Batch 1,400  of  2,505.    Elapsed: 0:02:35.\n",
            "  Batch 1,440  of  2,505.    Elapsed: 0:02:40.\n",
            "  Batch 1,480  of  2,505.    Elapsed: 0:02:44.\n",
            "  Batch 1,520  of  2,505.    Elapsed: 0:02:49.\n",
            "  Batch 1,560  of  2,505.    Elapsed: 0:02:53.\n",
            "  Batch 1,600  of  2,505.    Elapsed: 0:02:58.\n",
            "  Batch 1,640  of  2,505.    Elapsed: 0:03:03.\n",
            "  Batch 1,680  of  2,505.    Elapsed: 0:03:07.\n",
            "  Batch 1,720  of  2,505.    Elapsed: 0:03:12.\n",
            "  Batch 1,760  of  2,505.    Elapsed: 0:03:16.\n",
            "  Batch 1,800  of  2,505.    Elapsed: 0:03:21.\n",
            "  Batch 1,840  of  2,505.    Elapsed: 0:03:26.\n",
            "  Batch 1,880  of  2,505.    Elapsed: 0:03:30.\n",
            "  Batch 1,920  of  2,505.    Elapsed: 0:03:35.\n",
            "  Batch 1,960  of  2,505.    Elapsed: 0:03:40.\n",
            "  Batch 2,000  of  2,505.    Elapsed: 0:03:44.\n",
            "  Batch 2,040  of  2,505.    Elapsed: 0:03:49.\n",
            "  Batch 2,080  of  2,505.    Elapsed: 0:03:53.\n",
            "  Batch 2,120  of  2,505.    Elapsed: 0:03:58.\n",
            "  Batch 2,160  of  2,505.    Elapsed: 0:04:03.\n",
            "  Batch 2,200  of  2,505.    Elapsed: 0:04:07.\n",
            "  Batch 2,240  of  2,505.    Elapsed: 0:04:12.\n",
            "  Batch 2,280  of  2,505.    Elapsed: 0:04:17.\n",
            "  Batch 2,320  of  2,505.    Elapsed: 0:04:21.\n",
            "  Batch 2,360  of  2,505.    Elapsed: 0:04:26.\n",
            "  Batch 2,400  of  2,505.    Elapsed: 0:04:30.\n",
            "  Batch 2,440  of  2,505.    Elapsed: 0:04:35.\n",
            "  Batch 2,480  of  2,505.    Elapsed: 0:04:40.\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epoch took: 0:04:43\n",
            "\n",
            "Running Validation...\n",
            "Exact match: 25.4, F1 score: 63.46824086073314\n",
            "\n",
            "  Validation took: 0:04:36\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:18:16 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "import random,time\n",
        "import numpy as np\n",
        "\n",
        "# to reproduce results\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "#storing all training and validation stats\n",
        "stats = []\n",
        "\n",
        "\n",
        "#to measure total training time\n",
        "total_train_time_start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(' ')\n",
        "    print(f'=====Epoch {epoch + 1}=====')\n",
        "    print('Training....')\n",
        "\n",
        "    # ===============================\n",
        "    #    Train\n",
        "    # ===============================\n",
        "    # measure how long training epoch takes\n",
        "    t0 = time.time()\n",
        "\n",
        "    training_loss = 0\n",
        "    # loop through train data\n",
        "    model.train()\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "        # we will print train time in every 40 epochs\n",
        "        if step%40 == 0 and not step == 0:\n",
        "              elapsed_time = format_time(time.time() - t0)\n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed_time))\n",
        "\n",
        "\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "\n",
        "\n",
        "        #set gradients to zero\n",
        "        model.zero_grad()\n",
        "\n",
        "        result = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,\n",
        "                        start_positions = start_positions,\n",
        "                        end_positions = end_positions,\n",
        "                        return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "\n",
        "        #accumulate the loss over batches so that we can calculate avg loss at the end\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        #perform backward prorpogation\n",
        "        loss.backward()\n",
        "\n",
        "        # update the gradients\n",
        "        optimizer.step()\n",
        "\n",
        "    # calculate avg loss\n",
        "    avg_train_loss = training_loss/len(train_dataloader)\n",
        "\n",
        "    # calculates training time\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    #    Validation\n",
        "    # ===============================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    start_logits,end_logits = [],[]\n",
        "    for step,batch in enumerate(eval_dataloader):\n",
        "\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "             result = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,return_dict=True)\n",
        "\n",
        "\n",
        "\n",
        "        start_logits.append(result.start_logits.cpu().numpy())\n",
        "        end_logits.append(result.end_logits.cpu().numpy())\n",
        "\n",
        "\n",
        "    start_logits = np.concatenate(start_logits)\n",
        "    end_logits = np.concatenate(end_logits)\n",
        "    # start_logits = start_logits[: len(val_dataset)]\n",
        "    # end_logits = end_logits[: len(val_dataset)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # calculating metrics\n",
        "    answers,metrics_ = predict_answers_and_evaluate(start_logits,end_logits,validation_processed_dataset,dataset[\"validation\"])\n",
        "    print(f'Exact match: {metrics_[\"exact_match\"]}, F1 score: {metrics_[\"f1\"]}')\n",
        "\n",
        "\n",
        "    print('')\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_train_time_start)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3Rp6ayvF1Vx"
      },
      "source": [
        "**Note: You can train for more epochs with full data which will provide us a better result**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 2500666,
          "sourceId": 4243282,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30260,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "017f094634cd447d936595b0ade9d42e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40609dfb82dc4de9a5961c9a4fcbef1b",
              "IPY_MODEL_e6fea980df334b52a6aaf670e1fbf5a8",
              "IPY_MODEL_c43f416ba6dd4a0caa9c12f0e71b30df"
            ],
            "layout": "IPY_MODEL_cfba5bf272a945568842483b95782ea7"
          }
        },
        "40609dfb82dc4de9a5961c9a4fcbef1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f7a2a57b9f74d7aa0b25c8af5e67608",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1b8c5292fa2c4a83854d91f26b104dac",
            "value": "Map:â€‡100%"
          }
        },
        "e6fea980df334b52a6aaf670e1fbf5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d65c68975964edbbb8a1cc40c164846",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8f02dd53b8a4714a7cb2cce1eb831d0",
            "value": 500
          }
        },
        "c43f416ba6dd4a0caa9c12f0e71b30df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb0608734b154bf58858c5c2707ad00f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fc112def60624c37be1d287ba61f0ff8",
            "value": "â€‡500/500â€‡[00:00&lt;00:00,â€‡1145.37â€‡examples/s]"
          }
        },
        "cfba5bf272a945568842483b95782ea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f7a2a57b9f74d7aa0b25c8af5e67608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8c5292fa2c4a83854d91f26b104dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d65c68975964edbbb8a1cc40c164846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8f02dd53b8a4714a7cb2cce1eb831d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb0608734b154bf58858c5c2707ad00f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc112def60624c37be1d287ba61f0ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}